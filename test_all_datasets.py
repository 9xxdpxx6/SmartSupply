#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏ –ø–æ–¥–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
"""

import os
import sys
import pandas as pd
import json
from datetime import datetime
from typing import Dict, Any, List
import logging

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ app
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from app.preprocessing import parse_and_process
from app.train import train_prophet
from app.predict import predict_prophet

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def analyze_dataset_characteristics(csv_path: str) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞."""
    logger.info(f"\n{'='*80}")
    logger.info(f"üìä –ê–ù–ê–õ–ò–ó –î–ê–¢–ê–°–ï–¢–ê: {csv_path}")
    logger.info(f"{'='*80}")
    
    df = pd.read_csv(csv_path, nrows=1000)  # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ 1000 —Å—Ç—Ä–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç
    has_order_date = 'order_date' in df.columns
    has_sale_date = 'Sale_Date' in df.columns
    
    if has_order_date:
        date_col = 'order_date'
        qty_col = 'qty_ordered'
        cat_col = 'category'
    elif has_sale_date:
        date_col = 'Sale_Date'
        qty_col = 'Quantity_Sold'
        cat_col = 'Product_Category'
    else:
        logger.error(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞: {csv_path}")
        return {}
    
    # –ß–∏—Ç–∞–µ–º –ø–æ–ª–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    df_full = pd.read_csv(csv_path)
    df_full[date_col] = pd.to_datetime(df_full[date_col], errors='coerce')
    df_full = df_full.dropna(subset=[date_col])
    
    # –ê–Ω–∞–ª–∏–∑
    stats = {
        'total_rows': len(df_full),
        'date_min': df_full[date_col].min().isoformat(),
        'date_max': df_full[date_col].max().isoformat(),
        'date_span_days': (df_full[date_col].max() - df_full[date_col].min()).days,
        'unique_dates': df_full[date_col].nunique(),
        'total_sales': float(df_full[qty_col].sum()),
        'mean_sales': float(df_full[qty_col].mean()),
        'std_sales': float(df_full[qty_col].std()),
        'cv': float(df_full[qty_col].std() / df_full[qty_col].mean()) if df_full[qty_col].mean() > 0 else 0.0,
    }
    
    if cat_col in df_full.columns:
        stats['unique_categories'] = df_full[cat_col].nunique()
        stats['categories'] = df_full[cat_col].value_counts().head(10).to_dict()
    
    logger.info(f"   –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {stats['total_rows']:,}")
    logger.info(f"   –ü–µ—Ä–∏–æ–¥: {stats['date_min']} - {stats['date_max']} ({stats['date_span_days']} –¥–Ω–µ–π)")
    logger.info(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–∞—Ç: {stats['unique_dates']}")
    logger.info(f"   –í—Å–µ–≥–æ –ø—Ä–æ–¥–∞–∂: {stats['total_sales']:,.0f}")
    logger.info(f"   –°—Ä–µ–¥–Ω–µ–µ: {stats['mean_sales']:.2f}, Std: {stats['std_sales']:.2f}")
    logger.info(f"   –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏ (CV): {stats['cv']:.2f}")
    
    if stats['cv'] > 1.0:
        logger.warning(f"   ‚ö†Ô∏è –í–´–°–û–ö–ê–Ø –í–û–õ–ê–¢–ò–õ–¨–ù–û–°–¢–¨ (CV={stats['cv']:.2f})")
    elif stats['cv'] > 0.5:
        logger.info(f"   ‚ÑπÔ∏è –£–º–µ—Ä–µ–Ω–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å (CV={stats['cv']:.2f})")
    else:
        logger.info(f"   ‚úì –°—Ç–∞–±–∏–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (CV={stats['cv']:.2f})")
    
    if 'unique_categories' in stats:
        logger.info(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏–π: {stats['unique_categories']}")
    
    return stats


def test_dataset(
    csv_path: str,
    dataset_name: str,
    output_dir: str = "test_results"
) -> Dict[str, Any]:
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –æ–¥–∏–Ω –¥–∞—Ç–∞—Å–µ—Ç —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø–æ–¥–±–æ—Ä–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."""
    logger.info(f"\n{'='*80}")
    logger.info(f"üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï: {dataset_name}")
    logger.info(f"{'='*80}")
    
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs("data/processed", exist_ok=True)
    
    results = {
        'dataset_name': dataset_name,
        'csv_path': csv_path,
        'timestamp': datetime.now().isoformat(),
        'preprocessing': {},
        'training': {},
        'errors': []
    }
    
    try:
        # 1. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        logger.info("\nüìù –®–ê–ì 1: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        base_name = os.path.splitext(os.path.basename(csv_path))[0]
        out_shop_csv = f"data/processed/{base_name}_shop.csv"
        out_category_csv = f"data/processed/{base_name}_category.csv"
        out_product_csv = f"data/processed/{base_name}_product.csv"
        
        preprocess_result = parse_and_process(
            csv_path,
            out_shop_csv,
            out_category_csv,
            out_product_csv=out_product_csv,
            force_weekly=False
        )
        
        results['preprocessing'] = {
            'success': True,
            'shop_csv': preprocess_result['shop_csv'],
            'category_csv': preprocess_result['category_csv'],
            'stats': preprocess_result['stats']
        }
        logger.info(f"‚úì –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        logger.info(f"  Shop-level: {preprocess_result['stats']['shop_data_rows']} —Å—Ç—Ä–æ–∫")
        logger.info(f"  Category-level: {preprocess_result['stats']['category_data_rows']} —Å—Ç—Ä–æ–∫")
        
        # 2. –û–±—É—á–µ–Ω–∏–µ shop-level –º–æ–¥–µ–ª–∏ —Å auto_tune
        logger.info("\nüéØ –®–ê–ì 2: –û–±—É—á–µ–Ω–∏–µ shop-level –º–æ–¥–µ–ª–∏ —Å auto_tune...")
        shop_model_path = f"models/test_{dataset_name}_shop.pkl"
        os.makedirs(os.path.dirname(shop_model_path), exist_ok=True)
        
        train_result = train_prophet(
            shop_csv_path=out_shop_csv,
            model_out_path=shop_model_path,
            include_regressors=False,
            log_transform=False,
            interval_width=0.95,
            holdout_frac=0.2,
            changepoint_prior_scale=0.01,
            seasonality_prior_scale=10.0,
            seasonality_mode='additive',
            auto_tune=True,  # –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –ü–û–î–ë–û–† –ü–ê–†–ê–ú–ï–¢–†–û–í
            skip_holdout=False,
            filter_column=None,
            filter_value=None
        )
        
        results['training'] = {
            'success': True,
            'model_path': train_result['model_path'],
            'metrics': train_result['metrics'],
            'train_range': train_result['train_range'],
            'test_range': train_result['test_range'],
            'n_train': train_result['n_train'],
            'n_test': train_result['n_test']
        }
        
        logger.info(f"‚úì –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
        if train_result['metrics'].get('mape') is not None:
            logger.info(f"  MAPE: {train_result['metrics']['mape']:.2f}%")
            logger.info(f"  MAE: {train_result['metrics']['mae']:.2f}")
            logger.info(f"  RMSE: {train_result['metrics']['rmse']:.2f}")
        
        # 3. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–∞
        logger.info("\nüîÆ –®–ê–ì 3: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–∞ –Ω–∞ 30 –¥–Ω–µ–π...")
        forecast_df = predict_prophet(
            model_path=shop_model_path,
            horizon_days=30,
            log_transform=train_result['metrics'].get('log_transform', False),
            regressor_fill_method='forward',
            smooth_transition=False
        )
        
        results['forecast'] = {
            'success': True,
            'n_predictions': len(forecast_df),
            'forecast_mean': float(forecast_df['yhat'].mean()),
            'forecast_std': float(forecast_df['yhat'].std()),
            'forecast_min': float(forecast_df['yhat'].min()),
            'forecast_max': float(forecast_df['yhat'].max())
        }
        
        logger.info(f"‚úì –ü—Ä–æ–≥–Ω–æ–∑ —Å–æ–∑–¥–∞–Ω: {len(forecast_df)} –¥–Ω–µ–π")
        logger.info(f"  –°—Ä–µ–¥–Ω–µ–µ: {results['forecast']['forecast_mean']:.2f}")
        
        # 4. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if preprocess_result['stats']['unique_categories'] > 0:
            logger.info("\nüì¶ –®–ê–ì 4: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π...")
            
            # –ë–µ—Ä–µ–º —Ç–æ–ø-3 –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ –æ–±—ä–µ–º—É –ø—Ä–æ–¥–∞–∂
            category_df = pd.read_csv(out_category_csv)
            category_totals = category_df.groupby('category')['y'].sum().sort_values(ascending=False)
            top_categories = category_totals.head(3).index.tolist()
            
            results['category_tests'] = []
            
            for cat_name in top_categories:
                try:
                    logger.info(f"  –¢–µ—Å—Ç–∏—Ä—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é: {cat_name}...")
                    cat_model_path = f"models/test_{dataset_name}_category_{cat_name.replace(' ', '_')}.pkl"
                    
                    cat_train_result = train_prophet(
                        shop_csv_path=out_category_csv,
                        model_out_path=cat_model_path,
                        include_regressors=False,
                        log_transform=False,
                        interval_width=0.95,
                        holdout_frac=0.2,
                        changepoint_prior_scale=0.01,
                        seasonality_prior_scale=10.0,
                        seasonality_mode='additive',
                        auto_tune=True,  # –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –ü–û–î–ë–û–†
                        skip_holdout=False,
                        filter_column='category',
                        filter_value=cat_name
                    )
                    
                    cat_result = {
                        'category': cat_name,
                        'success': True,
                        'metrics': cat_train_result['metrics'],
                        'n_train': cat_train_result['n_train'],
                        'n_test': cat_train_result['n_test']
                    }
                    
                    if cat_train_result['metrics'].get('mape') is not None:
                        logger.info(f"    ‚úì MAPE: {cat_train_result['metrics']['mape']:.2f}%")
                    
                    results['category_tests'].append(cat_result)
                except Exception as e:
                    logger.error(f"    ‚ùå –û—à–∏–±–∫–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {cat_name}: {str(e)}")
                    results['category_tests'].append({
                        'category': cat_name,
                        'success': False,
                        'error': str(e)
                    })
        
        logger.info(f"\n‚úÖ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û: {dataset_name}")
        
    except Exception as e:
        logger.error(f"\n‚ùå –û–®–ò–ë–ö–ê –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ {dataset_name}: {str(e)}", exc_info=True)
        results['errors'].append(str(e))
        results['success'] = False
    
    return results


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤."""
    logger.info("="*80)
    logger.info("üöÄ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–û–í –° –ü–û–î–ë–û–†–û–ú –ü–ê–†–ê–ú–ï–¢–†–û–í")
    logger.info("="*80)
    
    # –°–ø–∏—Å–æ–∫ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    datasets = [
        {
            'name': 'sales_06_FY2020-21',
            'path': 'sales_06_FY2020-21.csv'
        },
        {
            'name': 'retail_sales_dataset',
            'path': 'retail_sales_dataset.csv'
        },
        {
            'name': 'customer_shopping_data',
            'path': 'customer_shopping_data.csv'
        }
    ]
    
    all_results = {}
    
    # –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    logger.info("\n" + "="*80)
    logger.info("üìä –≠–¢–ê–ü 1: –ê–ù–ê–õ–ò–ó –î–ê–¢–ê–°–ï–¢–û–í")
    logger.info("="*80)
    
    for dataset in datasets:
        if os.path.exists(dataset['path']):
            stats = analyze_dataset_characteristics(dataset['path'])
            all_results[dataset['name']] = {'stats': stats}
        else:
            logger.warning(f"‚ö†Ô∏è –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {dataset['path']}")
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    logger.info("\n" + "="*80)
    logger.info("üß™ –≠–¢–ê–ü 2: –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –° –ê–í–¢–û–ü–û–î–ë–û–†–û–ú –ü–ê–†–ê–ú–ï–¢–†–û–í")
    logger.info("="*80)
    
    for dataset in datasets:
        if not os.path.exists(dataset['path']):
            continue
        
        try:
            result = test_dataset(
                csv_path=dataset['path'],
                dataset_name=dataset['name'],
                output_dir="test_results"
            )
            all_results[dataset['name']]['test_result'] = result
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –¥–ª—è {dataset['name']}: {str(e)}", exc_info=True)
            all_results[dataset['name']]['error'] = str(e)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    logger.info("\n" + "="*80)
    logger.info("üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    logger.info("="*80)
    
    os.makedirs("test_results", exist_ok=True)
    results_file = f"test_results/test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False, default=str)
    
    logger.info(f"‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_file}")
    
    # –°–≤–æ–¥–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    logger.info("\n" + "="*80)
    logger.info("üìã –°–í–û–î–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    logger.info("="*80)
    
    for dataset_name, data in all_results.items():
        logger.info(f"\nüìä {dataset_name}:")
        if 'test_result' in data:
            tr = data['test_result']
            if tr.get('training', {}).get('success'):
                metrics = tr['training'].get('metrics', {})
                if metrics.get('mape') is not None:
                    logger.info(f"  Shop-level MAPE: {metrics['mape']:.2f}%")
                    logger.info(f"  MAE: {metrics['mae']:.2f}")
                    logger.info(f"  RMSE: {metrics['rmse']:.2f}")
                else:
                    logger.info(f"  ‚úì –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ (skip_holdout –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω)")
            
            if 'category_tests' in tr:
                logger.info(f"  –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã: {len(tr['category_tests'])}")
                for cat_test in tr['category_tests']:
                    if cat_test.get('success') and cat_test.get('metrics', {}).get('mape'):
                        logger.info(f"    - {cat_test['category']}: MAPE={cat_test['metrics']['mape']:.2f}%")
        elif 'error' in data:
            logger.error(f"  ‚ùå –û—à–∏–±–∫–∞: {data['error']}")
    
    logger.info("\n" + "="*80)
    logger.info("‚úÖ –í–°–ï –¢–ï–°–¢–´ –ó–ê–í–ï–†–®–ï–ù–´")
    logger.info("="*80)


if __name__ == "__main__":
    main()

