# file: app/train.py
import pandas as pd
import numpy as np
from prophet import Prophet
from sklearn.metrics import mean_absolute_error, mean_squared_error
import joblib
import warnings
import json
import os
import logging
from typing import Dict, Any, Optional

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def mape(actual: np.ndarray, predicted: np.ndarray) -> float:
    """
    Calculate Mean Absolute Percentage Error.
    
    Args:
        actual: Array of actual values
        predicted: Array of predicted values
        
    Returns:
        MAPE value as percentage
    """
    mask = actual != 0
    if mask.any():
        return np.mean(np.abs((actual[mask] - predicted[mask]) / actual[mask])) * 100
    else:
        return 0.0


def train_prophet(
    shop_csv_path: str, 
    model_out_path: str, 
    include_regressors: bool = False,
    log_transform: bool = False,
    interval_width: float = 0.95,
    holdout_frac: float = 0.2,
    changepoint_prior_scale: float = 0.01,  # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    seasonality_prior_scale: float = 10.0,
    seasonality_mode: str = 'additive',
    auto_tune: bool = False,
    skip_holdout: bool = False,  # –ï—Å–ª–∏ True, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –í–°–ï –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–±–µ–∑ —Ç–µ—Å—Ç–∞)
    filter_column: Optional[str] = None,  # 'category' –∏–ª–∏ 'product_id' –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    filter_value: Optional[str] = None  # –ó–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–Ω–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–ª–∏ ID —Ç–æ–≤–∞—Ä–∞)
) -> Dict[str, Any]:
    """
    Train a Prophet model on sales data (shop-level, category-level, or product-level).
    
    Args:
        shop_csv_path: Path to the CSV file (shop-level, category-level, or product-level)
        model_out_path: Path to save the trained model using joblib
        include_regressors: Whether to include avg_price and avg_discount as regressors
        log_transform: If True, apply log1p transformation to y before training
        interval_width: Confidence interval width for Prophet (default 0.95)
        holdout_frac: Fraction of data to use for testing (default 0.2)
        changepoint_prior_scale: Flexibility of automatic changepoint detection (default 0.05, higher = more flexible)
        seasonality_prior_scale: Strength of seasonality components (default 10.0, higher = stronger seasonality)
        seasonality_mode: 'additive' or 'multiplicative' (default 'additive')
        auto_tune: If True, perform automatic grid search to find best configuration
        filter_column: Optional column name for filtering ('category' or 'product_id')
        filter_value: Optional value to filter by (category name or product_id)
        
    Returns:
        Dictionary containing model path, metrics, and data ranges
    """
    logger.info(f"Starting model training from: {shop_csv_path}")
    
    # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è, –ª–æ–≥–∏—Ä—É–µ–º
    if filter_column and filter_value:
        logger.info(f"Filtering data: {filter_column} = '{filter_value}'")
    
    # Auto-tuning: perform grid search
    if auto_tune:
        logger.info("Auto-tuning enabled: performing grid search...")
        try:
            from app.tuning import grid_search_models
            
            analysis_dir = os.path.join(os.path.dirname(model_out_path) or 'models', '..', 'analysis')
            analysis_dir = os.path.normpath(analysis_dir)
            
            tuning_results = grid_search_models(
                shop_csv_path=shop_csv_path,
                holdout_frac=holdout_frac,
                output_dir=analysis_dir
            )
            
            if tuning_results.get('success', False):
                best_model = tuning_results['best_model']
                best_config = best_model.get('config', {})
                
                logger.info(f"Best model from grid search: {best_model['name']}")
                logger.info(f"Best metrics: MAPE={best_model['metrics']['mape']:.2f}%, "
                           f"Coverage={best_model['metrics']['coverage']*100:.1f}%")
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –ª—É—á—à–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
                if 'seasonality_prior_scale' in best_config:
                    seasonality_prior_scale = best_config['seasonality_prior_scale']
                if 'changepoint_prior_scale' in best_config:
                    changepoint_prior_scale = best_config['changepoint_prior_scale']
                if 'interval_width' in best_config:
                    interval_width = best_config['interval_width']
                if 'seasonality_mode' in best_config:
                    seasonality_mode = best_config['seasonality_mode']
                if 'include_regressors' in best_config:
                    include_regressors = best_config['include_regressors']
                
                logger.info(f"Using optimized parameters: seasonality_prior_scale={seasonality_prior_scale}, "
                           f"changepoint_prior_scale={changepoint_prior_scale}, interval_width={interval_width}, "
                           f"seasonality_mode={seasonality_mode}, include_regressors={include_regressors}")
            else:
                logger.warning("Grid search failed or returned no results, using default parameters")
        except Exception as e:
            logger.error(f"Auto-tuning failed: {str(e)}, falling back to default parameters")
            logger.error(f"Error details: {str(e)}", exc_info=True)
    
    # Read the shop CSV file
    if not os.path.exists(shop_csv_path):
        raise FileNotFoundError(f"Shop CSV file not found: {shop_csv_path}")
    
    df = pd.read_csv(shop_csv_path)
    logger.info(f"Loaded {len(df)} rows from CSV")
    
    # Verify required columns exist
    if 'ds' not in df.columns or 'y' not in df.columns:
        raise ValueError("CSV must contain 'ds' and 'y' columns")
    
    # Apply filtering if specified
    if filter_column and filter_value:
        if filter_column not in df.columns:
            raise ValueError(f"Filter column '{filter_column}' not found in CSV. Available columns: {list(df.columns)}")
        
        # Convert filter_value to appropriate type if needed
        if filter_column == 'product_id':
            # Try to match product_id as string or convert to match data type
            df_filtered = df[df[filter_column].astype(str) == str(filter_value)].copy()
        else:
            # For category, match as string
            df_filtered = df[df[filter_column].astype(str) == str(filter_value)].copy()
        
        if len(df_filtered) == 0:
            available_values = df[filter_column].unique()[:10]  # Show first 10
            raise ValueError(f"No data found for {filter_column}='{filter_value}'. "
                           f"Available values (first 10): {list(available_values)}")
        
        df = df_filtered
        logger.info(f"After filtering: {len(df)} rows for {filter_column}='{filter_value}'")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        # –î–ª—è –Ω–µ–¥–µ–ª—å–Ω–æ–π –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –º–µ–Ω—å—à–µ —Å—Ç—Ä–æ–∫, —á–µ–º –¥–ª—è –¥–Ω–µ–≤–Ω–æ–π
        min_required = 8 if 'ds' in df.columns and len(df) > 0 and (pd.to_datetime(df['ds'].max()) - pd.to_datetime(df['ds'].min())).days < 100 else 30
        if len(df) < min_required:
            raise ValueError(f"‚ö†Ô∏è –ù–ï–î–û–°–¢–ê–¢–û–ß–ù–û –î–ê–ù–ù–´–• –¥–ª—è {filter_column}='{filter_value}': {len(df)} —Å—Ç—Ä–æ–∫ "
                           f"(–º–∏–Ω–∏–º—É–º {min_required} —Ç—Ä–µ–±—É–µ—Ç—Å—è). "
                           f"–î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å –º–∞–ª—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö –ø–æ–ø—Ä–æ–±—É–π—Ç–µ:\n"
                           f"1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å shop-level –ø—Ä–æ–≥–Ω–æ–∑ (–±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π)\n"
                           f"2. –£–≤–µ–ª–∏—á–∏—Ç—å –ø–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö\n"
                           f"3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å skip_holdout=True –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö")
    
    # Prepare the data
    df = df.copy()
    df['ds'] = pd.to_datetime(df['ds'])
    df = df.sort_values('ds').reset_index(drop=True)
    
    # Save original y values before any transformation
    original_y = df['y'].copy()
    
    # Split data by time: train on first (1-holdout_frac), test on last holdout_frac
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ train/test –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
    if skip_holdout:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –í–°–ï –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–±–µ–∑ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞)
        logger.info("skip_holdout=True: Using ALL data for training (no test set split)")
        df_train = df.copy()
        df_test = pd.DataFrame(columns=df.columns)  # –ü—É—Å—Ç–æ–π test set
        n_train = len(df_train)
        n_test = 0
        if n_train < 30:
            raise ValueError(f"Insufficient data for training: {n_train} rows (minimum 30 required)")
        logger.info(f"Training on ALL {n_train} days ({df_train['ds'].min().date()} to {df_train['ds'].max().date()})")
        logger.info("‚ö†Ô∏è No test set - metrics will not be calculated. Use for production forecasts.")
    else:
        n_total = len(df)
        n_train = int(n_total * (1 - holdout_frac))
        n_test = n_total - n_train
        
        if n_train < 30:
            raise ValueError(f"Insufficient data for training: {n_train} rows (minimum 30 required)")
        
        df_train = df.iloc[:n_train].copy()
        df_test = df.iloc[n_train:].copy()
    
    # Apply log transformation if requested (after split to preserve original test values)
    # Always save original test y values for metrics calculation
    df_test_original_y = df_test['y'].copy()
    
    if log_transform:
        logger.info("Applying log1p transformation to target variable")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ—Å–ª–µ log_transform –æ—Å—Ç–∞–Ω–µ—Ç—Å—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
        # log1p(0) = 0, —Ç–∞–∫ —á—Ç–æ –Ω—É–ª–∏ –æ—Å—Ç–∞—é—Ç—Å—è –Ω—É–ª—è–º–∏, –Ω–æ Prophet –º–æ–∂–µ—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –Ω—É–ª–µ–π
        non_zero_before = (df_train['y'] > 0).sum()
        if non_zero_before < 2:
            raise ValueError(f"‚ö†Ô∏è –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ {filter_column}='{filter_value}' –æ—Å—Ç–∞–ª–æ—Å—å "
                           f"—Ç–æ–ª—å–∫–æ {non_zero_before} –Ω–µ–Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –æ–±—É—á–∞—é—â–µ–º –Ω–∞–±–æ—Ä–µ ({len(df_train)} –≤—Å–µ–≥–æ). "
                           f"Prophet –Ω–µ –º–æ–∂–µ—Ç –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ —Ç–∞–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö.\n"
                           f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:\n"
                           f"1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ shop-level –ø—Ä–æ–≥–Ω–æ–∑ –≤–º–µ—Å—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω–æ–≥–æ\n"
                           f"2. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –±–µ–∑ log-transform (—Å–Ω–∏–º–∏—Ç–µ –≥–∞–ª–æ—á–∫—É)\n"
                           f"3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ skip_holdout=True –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö\n"
                           f"4. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—ã–±—Ä–∞–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è –∏–º–µ–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–æ–¥–∞–∂")
        
        df_train['y'] = np.log1p(df_train['y'])
        df_test['y'] = np.log1p(df_test['y'])
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ—Å–ª–µ log_transform –Ω–µ –ø–æ—è–≤–∏–ª–æ—Å—å NaN
        nan_count_train = df_train['y'].isna().sum()
        if nan_count_train > 0:
            logger.warning(f"–ü–æ—Å–ª–µ log_transform –ø–æ—è–≤–∏–ª–æ—Å—å {nan_count_train} NaN –∑–Ω–∞—á–µ–Ω–∏–π, –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ 0")
            df_train['y'] = df_train['y'].fillna(0)
        
        nan_count_test = df_test['y'].isna().sum()
        if nan_count_test > 0 and len(df_test) > 0:
            logger.warning(f"–ü–æ—Å–ª–µ log_transform –≤ —Ç–µ—Å—Ç–µ –ø–æ—è–≤–∏–ª–æ—Å—å {nan_count_test} NaN –∑–Ω–∞—á–µ–Ω–∏–π, –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ 0")
            df_test['y'] = df_test['y'].fillna(0)
    
    
    train_range = {
        'start': df_train['ds'].min().isoformat(),
        'end': df_train['ds'].max().isoformat()
    }
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ test_range: –µ—Å–ª–∏ skip_holdout=True, df_test –ø—É—Å—Ç–æ–π
    if skip_holdout or len(df_test) == 0:
        test_range = {
            'start': None,
            'end': None
        }
        logger.info(f"Train period: {train_range['start']} to {train_range['end']} ({n_train} rows)")
        logger.info(f"Test period: N/A (skip_holdout=True, using all data for training)")
    else:
        test_range = {
            'start': df_test['ds'].min().isoformat(),
            'end': df_test['ds'].max().isoformat()
        }
        logger.info(f"Train period: {train_range['start']} to {train_range['end']} ({n_train} rows)")
        logger.info(f"Test period: {test_range['start']} to {test_range['end']} ({n_test} rows)")
    
    # Prepare data for Prophet
    prophet_cols = ['ds', 'y']
    if include_regressors:
        if 'avg_price' not in df.columns or 'avg_discount' not in df.columns:
            logger.warning("Regressors requested but avg_price/avg_discount not found in CSV. Proceeding without regressors.")
            include_regressors = False
        else:
            prophet_cols.extend(['avg_price', 'avg_discount'])
    
    df_prophet_train = df_train[prophet_cols].copy()
    
    # –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –í–û–õ–ê–¢–ò–õ–¨–ù–û–°–¢–ò –î–ê–ù–ù–´–• (–ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π)
    # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏ (CV) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
    train_values = df_prophet_train['y'].values
    train_values_nonzero = train_values[train_values > 0]
    
    if len(train_values_nonzero) > 1:
        mean_val = np.mean(train_values_nonzero)
        std_val = np.std(train_values_nonzero)
        cv = (std_val / mean_val) if mean_val > 0 else 0.0  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏
    else:
        cv = 0.0
        mean_val = 0.0
        std_val = 0.0
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
    is_highly_volatile = cv > 1.0  # CV > 1.0 –æ–∑–Ω–∞—á–∞–µ—Ç –æ—á–µ–Ω—å –≤—ã—Å–æ–∫—É—é –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
    is_moderately_volatile = cv > 0.5  # CV > 0.5 –æ–∑–Ω–∞—á–∞–µ—Ç —É–º–µ—Ä–µ–Ω–Ω—É—é –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
    
    logger.info(f"üìä –ê–Ω–∞–ª–∏–∑ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö:")
    logger.info(f"   –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏ (CV): {cv:.2f}")
    logger.info(f"   –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {mean_val:.2f}")
    logger.info(f"   –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {std_val:.2f}")
    
    if is_highly_volatile:
        logger.warning(f"‚ö†Ô∏è –û–ë–ù–ê–†–£–ñ–ï–ù–ê –í–´–°–û–ö–ê–Ø –í–û–õ–ê–¢–ò–õ–¨–ù–û–°–¢–¨ (CV={cv:.2f} > 1.0)")
        logger.warning("   –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    elif is_moderately_volatile:
        logger.info(f"‚ÑπÔ∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —É–º–µ—Ä–µ–Ω–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å (CV={cv:.2f} > 0.5)")
    
    # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π/—Ç–æ–≤–∞—Ä–æ–≤: –ø—Ä–æ–≤–µ—Ä—è–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –º–Ω–æ–≥–æ –Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    if filter_column is not None:
        zero_count = (df_prophet_train['y'] == 0).sum()
        zero_percent = (zero_count / len(df_prophet_train)) * 100 if len(df_prophet_train) > 0 else 0
        
        # –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å >30% –Ω—É–ª–µ–π
        # –ù–û –¥–ª—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –º–æ–∂–µ—Ç —É—Ö—É–¥—à–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑ - –ø—Ä–∏–º–µ–Ω—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ
        if zero_percent > 30:
            logger.warning(f"‚ö†Ô∏è –ú–ù–û–ì–û –ù–£–õ–ï–í–´–• –ó–ù–ê–ß–ï–ù–ò–ô: {zero_percent:.1f}% ({zero_count} –∏–∑ {len(df_prophet_train)})")
            logger.warning("–≠—Ç–æ –º–æ–∂–µ—Ç —Å–∏–ª—å–Ω–æ —É—Ö—É–¥—à–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑. Prophet –ø–ª–æ—Ö–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –Ω—É–∂–µ–Ω –ª–∏ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
            use_aggressive = zero_percent > 50
            
            # –î–õ–Ø –í–û–õ–ê–¢–ò–õ–¨–ù–´–• –î–ê–ù–ù–´–•: –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ
            if is_highly_volatile:
                logger.info("‚ö†Ô∏è –í–û–õ–ê–¢–ò–õ–¨–ù–´–ï –î–ê–ù–ù–´–ï: –º–∏–Ω–∏–º–∏–∑–∏—Ä—É—é —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏")
                use_aggressive = False  # –ù–µ –ø—Ä–∏–º–µ–Ω—è–µ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –¥–ª—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            
            if use_aggressive:
                logger.warning("‚ö†Ô∏è –û–ß–ï–ù–¨ –†–ê–ó–†–ï–ñ–ï–ù–ù–´–ï –î–ê–ù–ù–´–ï! –ü—Ä–∏–º–µ–Ω—è–µ–º –ê–ì–†–ï–°–°–ò–í–ù–£–Æ –æ–±—Ä–∞–±–æ—Ç–∫—É...")
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö (–∑–∞–º–µ–Ω–∞ –Ω—É–ª–µ–π –Ω–∞ –º–µ–¥–∏–∞–Ω—ã, —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ)
            # –ù–æ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ –æ—á–µ–Ω—å –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ã
            if not is_highly_volatile or zero_percent > 70:
                try:
                    from app.preprocessing import _improve_data_quality
                    df_prophet_train = _improve_data_quality(df_prophet_train, aggressive=use_aggressive)
                    logger.info("–£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ–Ω–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    zero_count_after = (df_prophet_train['y'] == 0).sum()
                    zero_percent_after = (zero_count_after / len(df_prophet_train)) * 100 if len(df_prophet_train) > 0 else 0
                    logger.info(f"–ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {zero_percent_after:.1f}% –Ω—É–ª–µ–π (–±—ã–ª–æ {zero_percent:.1f}%)")
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
            else:
                logger.info("‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞—é —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –¥–ª—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö - —Å–æ—Ö—Ä–∞–Ω—è—é –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å")
        
        if zero_percent > 70:
            logger.error(f"‚ö†Ô∏è –ö–†–ò–¢–ò–ß–ï–°–ö–ò –ú–ù–û–ì–û –ù–£–õ–ï–ô: {zero_percent:.1f}%!")
            logger.error("Prophet –º–æ–∂–µ—Ç –¥–∞—Ç—å –ø–ª–æ—Ö–æ–π –ø—Ä–æ–≥–Ω–æ–∑ –¥–∞–∂–µ –±–µ–∑ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏.")
            logger.error("–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ shop-level –ø—Ä–æ–≥–Ω–æ–∑ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç–µ –µ–≥–æ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ.")
        
        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ skip_holdout –¥–ª—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        if zero_percent > 40 and not skip_holdout:
            logger.warning(f"‚ö†Ô∏è –í–ê–ñ–ù–û: –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å {zero_percent:.1f}% –Ω—É–ª–µ–π —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è:")
            logger.warning("  1. –í–∫–ª—é—á–∏—Ç—å 'skip_holdout=True' (–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –í–°–ï–• –¥–∞–Ω–Ω—ã—Ö)")
            logger.warning("  2. –ò–ª–∏ —É–º–µ–Ω—å—à–∏—Ç—å holdout_frac –¥–æ 0.05-0.1")
            logger.warning("  3. –ò–Ω–∞—á–µ –º–æ–∂–µ—Ç –æ—Å—Ç–∞—Ç—å—Å—è —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
    
    # Validate seasonality_mode
    if seasonality_mode not in ['additive', 'multiplicative']:
        raise ValueError(f"seasonality_mode must be 'additive' or 'multiplicative', got '{seasonality_mode}'")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è yearly seasonality
    # Prophet —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç –º–∏–Ω–∏–º—É–º 730 –¥–Ω–µ–π (2 –≥–æ–¥–∞) –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–π yearly seasonality
    days_span = (df['ds'].max() - df['ds'].min()).days
    use_yearly = days_span >= 730  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö >= 2 –ª–µ—Ç
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∞–≥—Ä–µ–≥–∞—Ü–∏—è weekly –∏–ª–∏ daily
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ä–µ–¥–Ω–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –¥–∞—Ç–∞–º–∏
    if len(df) > 1:
        df_sorted = df.sort_values('ds')
        time_diffs = df_sorted['ds'].diff().dropna()
        avg_days_between = time_diffs.median().total_seconds() / (24 * 3600) if len(time_diffs) > 0 else 1.0
        is_weekly_aggregated = avg_days_between >= 5.0  # –ï—Å–ª–∏ —Å—Ä–µ–¥–Ω–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª >= 5 –¥–Ω–µ–π, —ç—Ç–æ weekly
    else:
        is_weekly_aggregated = False
        avg_days_between = 1.0
    
    # –î–ª—è shop-level –¥–∞–Ω–Ω—ã—Ö —Ç–æ–∂–µ –ø—Ä–∏–º–µ–Ω—è–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
    # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π/—Ç–æ–≤–∞—Ä–æ–≤ –ø—Ä–∏–º–µ–Ω—è–µ–º –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    # –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ —É–∫–∞–∑–∞–ª —è–≤–Ω–æ –¥—Ä—É–≥–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    is_category_or_product = filter_column is not None
    if is_category_or_product:
        # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≥–∏–±–∫–æ—Å—Ç—å changepoints –µ—â–µ –±–æ–ª—å—à–µ –¥–ª—è —É–ª–∞–≤–ª–∏–≤–∞–Ω–∏—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ changepoints –±–µ–∑ seasonality –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        
        # –ê–î–ê–ü–¢–ò–í–ù–ê–Ø –ù–ê–°–¢–†–û–ô–ö–ê –î–õ–Ø –í–û–õ–ê–¢–ò–õ–¨–ù–´–• –î–ê–ù–ù–´–•
        if is_highly_volatile:
            # –î–ª—è –æ—á–µ–Ω—å –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–∏–±–∫–æ—Å—Ç—å
            if changepoint_prior_scale <= 0.01:
                changepoint_prior_scale = 0.5  # –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è –≥–∏–±–∫–æ—Å—Ç—å –¥–ª—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                logger.info(f"üî• –í–´–°–û–ö–ê–Ø –í–û–õ–ê–¢–ò–õ–¨–ù–û–°–¢–¨: —É–≤–µ–ª–∏—á–∏–≤–∞—é changepoint_prior_scale –¥–æ 0.5 –¥–ª—è –ª—É—á—à–µ–≥–æ —É–ª–∞–≤–ª–∏–≤–∞–Ω–∏—è –≤—Å–ø–ª–µ—Å–∫–æ–≤")
            elif changepoint_prior_scale < 0.3:
                changepoint_prior_scale = max(changepoint_prior_scale * 3.0, 0.3)
                logger.info(f"üî• –í–û–õ–ê–¢–ò–õ–¨–ù–û–°–¢–¨: —É–≤–µ–ª–∏—á–∏–≤–∞—é changepoint_prior_scale –¥–æ {changepoint_prior_scale}")
            
            # –î–ª—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º multiplicative —Ä–µ–∂–∏–º –µ—Å–ª–∏ –æ–Ω –Ω–µ –±—ã–ª —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω
            if seasonality_mode == 'additive' and not auto_tune:
                logger.info("üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –¥–ª—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å multiplicative —Ä–µ–∂–∏–º")
                # –ù–µ –º–µ–Ω—è–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏, —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–µ–º
        elif is_moderately_volatile:
            # –î–ª—è —É–º–µ—Ä–µ–Ω–Ω–æ –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö - —Å—Ä–µ–¥–Ω—è—è –≥–∏–±–∫–æ—Å—Ç—å
            if changepoint_prior_scale <= 0.01:
                changepoint_prior_scale = 0.3
                logger.info(f"üìà –£–º–µ—Ä–µ–Ω–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å: —É–≤–µ–ª–∏—á–∏–≤–∞—é changepoint_prior_scale –¥–æ 0.3")
            elif changepoint_prior_scale < 0.2:
                changepoint_prior_scale = max(changepoint_prior_scale * 2.0, 0.2)
                logger.info(f"üìà –£–º–µ—Ä–µ–Ω–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å: —É–≤–µ–ª–∏—á–∏–≤–∞—é changepoint_prior_scale –¥–æ {changepoint_prior_scale}")
        else:
            # –î–ª—è —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            if changepoint_prior_scale <= 0.01:
                changepoint_prior_scale = 0.25  # –í—ã—Å–æ–∫–∞—è –≥–∏–±–∫–æ—Å—Ç—å –¥–ª—è —É–ª–∞–≤–ª–∏–≤–∞–Ω–∏—è –≤—Å–ø–ª–µ—Å–∫–æ–≤ –∏ –ø–∞–¥–µ–Ω–∏–π
                logger.info("Category/product data: increasing changepoint_prior_scale to 0.25 for better volatility capture")
            elif changepoint_prior_scale < 0.2:
                changepoint_prior_scale = max(changepoint_prior_scale * 2.0, 0.2)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–∫–∞–∑–∞–ª –Ω–∏–∑–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                logger.info(f"Category/product data: increasing changepoint_prior_scale to {changepoint_prior_scale} for volatility")
        
        # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫–ª—é—á–∞–µ–º seasonality - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ changepoints
        seasonality_prior_scale = 0.1  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (—Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å –±—É–¥–µ—Ç –æ—Ç–∫–ª—é—á–µ–Ω–∞)
        logger.info("Category/product data: disabling seasonality completely, using only trend + flexible changepoints")
        
        # –£–º–µ–Ω—å—à–∞–µ–º interval_width –¥–ª—è –±–æ–ª–µ–µ —É–∑–∫–æ–≥–æ –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞
        if interval_width >= 0.95:
            interval_width = 0.80  # –ë–æ–ª–µ–µ —É–∑–∫–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            logger.info("Category/product data: reducing interval_width to 0.80 for narrower confidence interval")
    else:
        # –î–õ–Ø SHOP-LEVEL –î–ê–ù–ù–´–•: —Ç–∞–∫–∂–µ –ø—Ä–∏–º–µ–Ω—è–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        if is_highly_volatile:
            # –î–ª—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ã—Ö shop-level –¥–∞–Ω–Ω—ã—Ö —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º changepoint_prior_scale
            if changepoint_prior_scale <= 0.01:
                changepoint_prior_scale = 0.1  # –£–º–µ—Ä–µ–Ω–Ω–∞—è –≥–∏–±–∫–æ—Å—Ç—å –¥–ª—è shop-level
                logger.info(f"üî• –í–û–õ–ê–¢–ò–õ–¨–ù–´–ï SHOP-LEVEL –î–ê–ù–ù–´–ï: —É–≤–µ–ª–∏—á–∏–≤–∞—é changepoint_prior_scale –¥–æ 0.1")
            elif changepoint_prior_scale < 0.05:
                changepoint_prior_scale = max(changepoint_prior_scale * 2.0, 0.05)
                logger.info(f"üî• –í–û–õ–ê–¢–ò–õ–¨–ù–û–°–¢–¨: —É–≤–µ–ª–∏—á–∏–≤–∞—é changepoint_prior_scale –¥–æ {changepoint_prior_scale}")
        elif is_moderately_volatile:
            if changepoint_prior_scale <= 0.01:
                changepoint_prior_scale = 0.05
                logger.info(f"üìà –£–º–µ—Ä–µ–Ω–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å shop-level: —É–≤–µ–ª–∏—á–∏–≤–∞—é changepoint_prior_scale –¥–æ 0.05")
    
    if not use_yearly and days_span < 730:
        logger.warning(f"Data span ({days_span} days) < 730 days. Disabling yearly_seasonality for stability.")
        logger.info("Using only weekly_seasonality. This is recommended for datasets < 2 years.")
        logger.warning("‚ö†Ô∏è LONG-TERM FORECAST WARNING: For forecasts > 90 days with data < 2 years, "
                      "the model may show flat/cyclical patterns due to missing yearly_seasonality.")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ weekly seasonality
    # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π/—Ç–æ–≤–∞—Ä–æ–≤ –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫–ª—é—á–∞–µ–º seasonality - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ changepoints
    if filter_column is not None:
        # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫–ª—é—á–∞–µ–º weekly –∏ yearly seasonality
        # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –±—É–¥–µ—Ç —É–ª–∞–≤–ª–∏–≤–∞—Ç—å—Å—è —á–µ—Ä–µ–∑ –≥–∏–±–∫–∏–µ changepoints
        use_weekly_seasonality = False
        use_yearly = False  # Yearly –æ—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        logger.info(f"‚ö†Ô∏è –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π/—Ç–æ–≤–∞—Ä–æ–≤ –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫–ª—é—á–∞–µ–º —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å")
        logger.info("–í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –±—É–¥–µ—Ç —É–ª–∞–≤–ª–∏–≤–∞—Ç—å—Å—è —á–µ—Ä–µ–∑ –≥–∏–±–∫–∏–µ changepoints (changepoint_prior_scale={:.2f})".format(changepoint_prior_scale))
        logger.info("–≠—Ç–æ –¥–æ–ª–∂–Ω–æ —É—Å—Ç—Ä–∞–Ω–∏—Ç—å —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —É–ª–∞–≤–ª–∏–≤–∞—Ç—å –≤—Å–ø–ª–µ—Å–∫–∏ –∏ –ø–∞–¥–µ–Ω–∏—è")
    else:
        use_weekly_seasonality = True
        # use_yearly —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω –≤—ã—à–µ
    
    # Initialize Prophet model with configurable hyperparameters
    model = Prophet(
        weekly_seasonality=use_weekly_seasonality,  # –û—Ç–∫–ª—é—á–µ–Ω–æ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        yearly_seasonality=use_yearly,  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        interval_width=interval_width,
        changepoint_prior_scale=changepoint_prior_scale,
        seasonality_prior_scale=seasonality_prior_scale,
        seasonality_mode=seasonality_mode
    )
    
    # –î–ª—è weekly –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º monthly seasonality - –æ–Ω–∞ —Å–æ–∑–¥–∞–µ—Ç —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    # –î–ª—è –¥–∞–Ω–Ω—ã—Ö >= 365 –¥–Ω–µ–π (–Ω–æ < 730): –¥–æ–±–∞–≤–ª—è–µ–º –º–µ—Å—è—á–Ω—É—é —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å –∫–∞–∫ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å
    # –ù–û —Ç–æ–ª—å–∫–æ –¥–ª—è daily –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –∏–ª–∏ shop-level –¥–∞–Ω–Ω—ã—Ö
    if days_span >= 365 and days_span < 730:
        # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π/—Ç–æ–≤–∞—Ä–æ–≤ –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º monthly seasonality
        if filter_column is not None:
            logger.info("Category/product data: skipping monthly seasonality (using only trend)")
        else:
            # –î–ª—è shop-level –¥–æ–±–∞–≤–ª—è–µ–º monthly seasonality
            logger.info(f"Data span ({days_span} days) >= 365 but < 730. Adding monthly seasonality.")
            model.add_seasonality(name='monthly', period=30.5, fourier_order=5)
    
    logger.info(f"Prophet model config: changepoint_prior_scale={changepoint_prior_scale}, "
                f"seasonality_prior_scale={seasonality_prior_scale}, seasonality_mode={seasonality_mode}")
    
    # Add regressors if requested
    if include_regressors:
        logger.info("Adding regressors: avg_price, avg_discount")
        model.add_regressor('avg_price')
        model.add_regressor('avg_discount')
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–∏
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–ª–∏–¥–Ω—ã—Ö (non-NaN) —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π
    valid_rows = df_prophet_train['y'].notna().sum()
    total_rows = len(df_prophet_train)
    
    if valid_rows < 2:
        error_msg = (
            f"‚ö†Ô∏è –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ü–æ—Å–ª–µ –≤—Å–µ—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π –æ—Å—Ç–∞–ª–æ—Å—å –º–µ–Ω—å—à–µ 2 –≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫ "
            f"–¥–ª—è –æ–±—É—á–µ–Ω–∏—è ({valid_rows} –∏–∑ {total_rows} —Å—Ç—Ä–æ–∫).\n\n"
            f"–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:\n"
            f"1. –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏/—Ç–æ–≤–∞—Ä–µ (>90%)\n"
            f"2. –ù–µ–¥–µ–ª—å–Ω–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è + —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—Å—Ç–∞–≤–∏–ª–∞ —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö "
            f"   (–Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 8-10 –Ω–µ–¥–µ–ª—å –¥–ª—è –Ω–µ–¥–µ–ª—å–Ω–æ–π –∞–≥—Ä–µ–≥–∞—Ü–∏–∏)\n"
            f"3. log_transform –≤ —Å–æ—á–µ—Ç–∞–Ω–∏–∏ —Å —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ —Å–æ–∑–¥–∞–ª –ø—Ä–æ–±–ª–µ–º—ã\n"
            f"4. holdout_frac —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n\n"
            f"–†–µ—à–µ–Ω–∏—è (–≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞):\n"
            f"1. ‚ùå –û–¢–ö–õ–Æ–ß–ò–¢–ï log-transform (—Å–Ω–∏–º–∏—Ç–µ –≥–∞–ª–æ—á–∫—É) - —ç—Ç–æ —á–∞—Å—Ç–æ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É\n"
            f"2. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ skip_holdout=True (–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –í–°–ï–• –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è)\n"
            f"3. ‚úÖ –ü–æ–ø—Ä–æ–±—É–π—Ç–µ daily –∞–≥—Ä–µ–≥–∞—Ü–∏—é –≤–º–µ—Å—Ç–æ weekly (–±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö)\n"
            f"4. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ shop-level –ø—Ä–æ–≥–Ω–æ–∑ (—Ä–∞–±–æ—Ç–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ –¥–ª—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π)\n"
            f"5. ‚úÖ –í—ã–±–µ—Ä–∏—Ç–µ –¥—Ä—É–≥—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–æ–¥–∞–∂"
        )
        
        if filter_column is not None:
            error_msg += f"\n\nüí° –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{filter_value}': –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º."
        
        raise ValueError(error_msg)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –Ω–µ–¥–µ–ª—å–Ω–æ–π –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
    if total_rows < 8 and filter_column is not None:
        logger.warning(f"‚ö†Ô∏è –û—á–µ–Ω—å –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö: {total_rows} —Å—Ç—Ä–æ–∫. –î–ª—è –Ω–µ–¥–µ–ª—å–Ω–æ–π –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 15-20 –Ω–µ–¥–µ–ª—å.")
    
    # Fit the model
    logger.info(f"Fitting Prophet model on {valid_rows} valid rows ({total_rows} total)...")
    try:
        model.fit(df_prophet_train)
        logger.info("Model fitted successfully")
    except Exception as e:
        error_str = str(e)
        if "less than 2 non-NaN rows" in error_str or "Dataframe has less than 2" in error_str:
            raise ValueError(
                f"‚ö†Ô∏è Prophet –Ω–µ –º–æ–∂–µ—Ç –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö.\n"
                f"–í–∞–ª–∏–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫: {valid_rows}, –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {total_rows}\n"
                f"–û—à–∏–±–∫–∞ Prophet: {error_str}\n\n"
                f"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ:\n"
                f"1. ‚ùå –û–¢–ö–õ–Æ–ß–ò–¢–ï log-transform (—Å–Ω–∏–º–∏—Ç–µ –≥–∞–ª–æ—á–∫—É)\n"
                f"2. ‚úÖ –í–∫–ª—é—á–∏—Ç–µ skip_holdout=True (–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö)\n"
                f"3. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ daily –∞–≥—Ä–µ–≥–∞—Ü–∏—é –≤–º–µ—Å—Ç–æ weekly\n"
                f"4. ‚úÖ –í—ã–±–µ—Ä–∏—Ç–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö\n"
                f"5. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ shop-level –ø—Ä–æ–≥–Ω–æ–∑ (—Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ)"
            ) from e
        else:
            raise
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ (–ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç—Å—è –µ—Å–ª–∏ skip_holdout)
    if skip_holdout:
        logger.info("Skipping metrics calculation (skip_holdout=True)")
        use_cv = False
        metrics_dict = {
            'mae': None,
            'rmse': None,
            'mape': None,
            'coverage': None,
            'log_transform': log_transform,
            'interval_width': interval_width,
            'holdout_frac': holdout_frac,
            'used_cross_validation': False,
            'changepoint_prior_scale': changepoint_prior_scale,
            'seasonality_prior_scale': seasonality_prior_scale,
            'seasonality_mode': seasonality_mode,
            'auto_tune': auto_tune,
            'skip_holdout': skip_holdout,
            'note': 'Model trained on ALL data. No test set metrics available. Ready for production forecasts.'
        }
    elif n_test < 7:
        # Check if test set is too small for proper evaluation
        use_cv = True
        logger.warning(f"Test set too small ({n_test} < 7). Using time-series cross-validation instead.")
    else:
        use_cv = False
    
    # Calculate metrics (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ skip_holdout)
    if not skip_holdout:
        if use_cv:
            # Time-series cross-validation
            logger.info("Performing time-series cross-validation...")
            from prophet.diagnostics import cross_validation, performance_metrics
            
            # Perform cross-validation with minimum training period of 30 days
            min_periods = min(30, n_train)
            cv_results = cross_validation(
                model, 
                initial=f'{min_periods} days',
                period='7 days',
                horizon='7 days'
            )
            
            cv_metrics = performance_metrics(cv_results)
            
            # Get actual and predicted values from CV
            actual_values = cv_results['y'].values
            predicted_values = cv_results['yhat'].values
            
            # Apply inverse transform if log_transform was used
            if log_transform:
                logger.info("Applying inverse log1p (expm1) transformation to predictions")
                actual_values = np.expm1(actual_values)
                predicted_values = np.expm1(predicted_values)
            
            # Ensure non-negative values for predictions (realistic constraint)
            # Sales cannot be negative, so clip negative predictions to 0
            n_negative_pred = (predicted_values < 0).sum()
            if n_negative_pred > 0:
                logger.info(f"Clamping {n_negative_pred} negative CV predictions to 0 (sales cannot be negative)")
                predicted_values = np.clip(predicted_values, 0, None)
            
            # Also clip actual values if they're negative (data quality issue)
            n_negative_actual = (actual_values < 0).sum()
            if n_negative_actual > 0:
                logger.warning(f"Found {n_negative_actual} negative actual values in CV (data quality issue)")
                actual_values = np.clip(actual_values, 0, None)
            
            # Calculate metrics
            mae_val = mean_absolute_error(actual_values, predicted_values)
            rmse_val = np.sqrt(mean_squared_error(actual_values, predicted_values))
            mape_val = mape(actual_values, predicted_values)
            
            logger.info(f"Cross-validation metrics: MAE={mae_val:.2f}, RMSE={rmse_val:.2f}, MAPE={mape_val:.2f}%")
        
        else:
            # Standard test set evaluation
            # Create future dataframe for test period
            periods = len(df_test)
            future = model.make_future_dataframe(periods=periods, freq='D')
            
            # Add regressors for future dates if needed
            if include_regressors:
                # Combine train and test regressors for complete coverage
                all_regressors = pd.concat([
                    df_train[['ds', 'avg_price', 'avg_discount']],
                    df_test[['ds', 'avg_price', 'avg_discount']]
                ], ignore_index=True)
                
                # Merge regressors
                future = future.merge(
                    all_regressors,
                    on='ds',
                    how='left'
                )
                
                # Forward-fill regressors for any missing dates
                for col in ['avg_price', 'avg_discount']:
                    if future[col].isna().any():
                        # Forward fill, then use last known value if still NaN
                        future[col] = future[col].ffill()
                        if future[col].isna().any():
                            last_known_value = all_regressors[col].iloc[-1] if not all_regressors.empty else 0
                            future[col] = future[col].fillna(last_known_value)
            
            # Make predictions
            logger.info(f"Generating predictions for {periods} periods...")
            forecast = model.predict(future)
            
            # Extract predictions for test period only
            test_mask = forecast['ds'] >= df_test['ds'].min()
            forecast_test = forecast[test_mask].copy()
            
            # Align actual and predicted values
            forecast_test = forecast_test.sort_values('ds')
            df_test_aligned = df_test.sort_values('ds')
            
            # Merge on ds to ensure alignment
            merged = forecast_test[['ds', 'yhat']].merge(
                df_test_aligned[['ds']],
                on='ds',
                how='inner'
            )
            
            # Get actual y values for metrics (original values if log_transform, transformed otherwise)
            if log_transform:
                # Use original y values before log transform
                merged = merged.merge(
                    pd.DataFrame({'ds': df_test['ds'].values, 'y_original': df_test_original_y.values}),
                    on='ds',
                    how='inner'
                )
                actual_values = merged['y_original'].values
                logger.info("Applying inverse log1p (expm1) transformation to predictions")
                predicted_values = np.expm1(merged['yhat'].values)
            else:
                # Use transformed y values
                merged = merged.merge(
                    df_test_aligned[['ds', 'y']],
                    on='ds',
                    how='inner'
                )
                actual_values = merged['y'].values
                predicted_values = merged['yhat'].values
            
            # Ensure non-negative values for predictions (realistic constraint)
            # Sales cannot be negative, so clip negative predictions to 0
            n_negative_pred = (predicted_values < 0).sum()
            if n_negative_pred > 0:
                logger.info(f"Clamping {n_negative_pred} negative predictions to 0 (sales cannot be negative)")
                predicted_values = np.clip(predicted_values, 0, None)
            
            # Also clip actual values if they're negative (data quality issue)
            n_negative_actual = (actual_values < 0).sum()
            if n_negative_actual > 0:
                logger.warning(f"Found {n_negative_actual} negative actual values (data quality issue)")
                actual_values = np.clip(actual_values, 0, None)
            
            # Calculate metrics
            mae_val = mean_absolute_error(actual_values, predicted_values)
            rmse_val = np.sqrt(mean_squared_error(actual_values, predicted_values))
            mape_val = mape(actual_values, predicted_values)
            
            logger.info(f"Test metrics: MAE={mae_val:.2f}, RMSE={rmse_val:.2f}, MAPE={mape_val:.2f}%")
            
            # Prepare metrics dictionary for test case
            metrics_dict = {
                'mae': float(mae_val),
                'rmse': float(rmse_val),
                'mape': float(mape_val),
                'log_transform': log_transform,
                'interval_width': interval_width,
                'holdout_frac': holdout_frac,
                'used_cross_validation': use_cv,
                'changepoint_prior_scale': changepoint_prior_scale,
                'seasonality_prior_scale': seasonality_prior_scale,
                'seasonality_mode': seasonality_mode,
                'auto_tune': auto_tune,
                'skip_holdout': skip_holdout
            }
    
    # Save the trained model
    model_dir = os.path.dirname(model_out_path)
    if model_dir:
        os.makedirs(model_dir, exist_ok=True)
    
    logger.info(f"Saving model to: {model_out_path}")
    joblib.dump(model, model_out_path)
    
    # Save metrics to JSON file
    metrics_path = model_out_path.replace('.pkl', '_metrics.json')
    logger.info(f"Saving metrics to: {metrics_path}")
    with open(metrics_path, 'w') as f:
        json.dump(metrics_dict, f, indent=2)
    
    # Prepare return dictionary
    results = {
        'model_path': model_out_path,
        'metrics': metrics_dict,
        'train_range': train_range,
        'test_range': test_range,
        'n_train': n_train,
        'n_test': n_test if not use_cv else len(cv_results) if use_cv else n_test
    }
    
    logger.info("Training completed successfully")
    
    return results


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Train Prophet model on shop-level sales data")
    parser.add_argument("shop_csv", help="Path to shop-level CSV file")
    parser.add_argument("model_out", help="Output path for trained model")
    parser.add_argument("--include-regressors", action="store_true", 
                       help="Include avg_price and avg_discount as regressors")
    parser.add_argument("--log-transform", action="store_true",
                       help="Apply log1p transformation to target variable")
    parser.add_argument("--interval-width", type=float, default=0.95,
                       help="Confidence interval width (default: 0.95)")
    parser.add_argument("--holdout-frac", type=float, default=0.2,
                       help="Fraction of data for testing (default: 0.2)")
    parser.add_argument("--auto-tune", action="store_true",
                       help="Perform automatic grid search to find best configuration")
    
    args = parser.parse_args()
    
    result = train_prophet(
        shop_csv_path=args.shop_csv,
        model_out_path=args.model_out,
        include_regressors=args.include_regressors,
        log_transform=args.log_transform,
        interval_width=args.interval_width,
        holdout_frac=args.holdout_frac,
        auto_tune=args.auto_tune
    )
    
    print("\n" + "="*60)
    print("Training completed successfully!")
    print("="*60)
    print(f"\nModel saved to: {result['model_path']}")
    print(f"Metrics saved to: {result['model_path'].replace('.pkl', '_metrics.json')}")
    print(f"\nTraining period: {result['train_range']['start']} to {result['train_range']['end']}")
    print(f"Training samples: {result['n_train']}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ test_range –Ω–∞ None (–∫–æ–≥–¥–∞ skip_holdout=True)
    test_start = result['test_range'].get('start') if result.get('test_range') else None
    test_end = result['test_range'].get('end') if result.get('test_range') else None
    if test_start and test_end:
        print(f"Test period: {test_start} to {test_end}")
        print(f"Test samples: {result['n_test']}")
    else:
        print(f"Test period: N/A (skip_holdout=True)")
        print(f"Test samples: 0")
    print(f"\nMetrics:")
    mae_val = result['metrics'].get('mae')
    rmse_val = result['metrics'].get('rmse')
    mape_val = result['metrics'].get('mape')
    
    if mae_val is not None:
        print(f"  MAE:  {mae_val:.4f}")
    else:
        print(f"  MAE:  N/A (skip_holdout=True)")
    
    if rmse_val is not None:
        print(f"  RMSE: {rmse_val:.4f}")
    else:
        print(f"  RMSE: N/A (skip_holdout=True)")
    
    if mape_val is not None:
        print(f"  MAPE: {mape_val:.2f}%")
    else:
        print(f"  MAPE: N/A (skip_holdout=True)")
    
    print(f"  Log transform: {result['metrics'].get('log_transform', False)}")
    print(f"  Used cross-validation: {result['metrics']['used_cross_validation']}")
